---
output:
  xaringan::moon_reader:
    css: ["default", "styling/sydney-fonts.css", "styling/sydney.css"]
    self_contained: false
    seal: true
    nature:
      beforeInit: ["styling/remark-zoom.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
      navigation:
        scroll: false
---

class: title-slide
background-image: url("styling/USydLogo-black.svg"), url("styling/title-image1.jpg")
background-position: 10% 90%, 100% 50%
background-size: 160px, 100% 100%

.content-box-purple[
# .black[Activity 2]
## Performance Evaluation of Classifications
### Dario Strbenac
### 29 June 2018
]

---

## Activity Overview

- Feature selection stability.

--

- Evaluation of overall error, sample-specific error, precision, recall.

--

- Comparison of DE-based and DD-based classifiers.

---

```{r, echo = FALSE}
library(knitr)
opts_knit[["set"]](root.dir = "/home/dario/Documents/tutorial/")
load("/home/dario/Documents/tutorial/data/classified.RData") # From AMLclassifiers.html tutorial.
```

---

## Feature Selection Stability

- If the genes being selected are the same ones in most of the cross-validations, then the classifier has good stability.

--

- `selectionPlot` provides a way to compare all pairs of gene selections within a classifier or between classifiers.

--

- Input is a list of `ClassifyResult` objects.

--

-  For 20 permutations and 5 folds, there are $^{100}C_2 = 4950$ overlaps to compute. Like `runTests`, `selectionPlot` may utilise multiple processors.

```{r, message = FALSE, echo = FALSE}
library(ClassifyR)
```

---

## Feature Selection Stability

```{r, fig.align = "center", fig.width = 14, fig.height = 4}
margins <- grid::unit(c(1, 1, 1, 1), "lines")
withinChoices <- selectionPlot(list(classifiedDM, classifiedDD),
                               xVariable = "selectionName", xLabel = "Selection Method",
                               columnVariable = "None",
                               boxFillColouring = "None", boxLineColouring = "None",
                               rotate90 = TRUE, margin = margins)
```

The DD feature selection is more stable than the DE selection used.

## Changing Elements of Saved Plots

- Almost all plots produced by ClassifyR are `ggplot` objects created by ggplot2.

- Such objects can be customised after creation because ggplot2 uses a 'painting-over' graphics model, unlike base R graphics.

```{r, fig.align = "center", fig.width = 14, fig.height = 4}
withinChoices + ggplot2::ggtitle("Chosen Gene Set Overlaps")
```

---

## Error / Accuracy of Predictions

- `calcCVperformance` calculates performance metrics for `ClassifyResult` objects. 12 different metrics can be calculated.

--

- Metrics are all applicable to data sets with two *or more* classes.

--

- `calcExternalPerformance` can be used on a pair of factor vectors of the same length.

--

```{r}
actualClasses <- factor(c("Yes", "Yes", "No", "No", "No"))
predictedClasses <- factor(c("Yes", "No", "No", "No", "No"))
calcExternalPerformance(actualClasses, predictedClasses, "error")
calcExternalPerformance(actualClasses, predictedClasses, "accuracy")
```

---

## Balanced Error Rate for Resistance Classification

- Class sizes of resistance data set are imbalanced. Errors should be summarised by the balanced error rate.

--

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "balanced error")
classifiedDM
classifiedDD <- calcCVperformance(classifiedDD, "balanced error")
```

---

## Distribution of Balanced Error Rate

The set of samples was predicted 20 times. `performancePlot` can be used to plot the distribution of a metric to see its stability.

```{r, fig.width = 14, fig.height = 4}
errorPlot <- performancePlot(list(classifiedDM, classifiedDD),
                             performanceName = "Balanced Error Rate",
                             boxFillColouring = "None", boxLineColouring = "None",
                             columnVariable = "None", title = "DV vs. DD Errors",
                             rotate90 = TRUE, margin = margins)
```

--

The predictions of the naive Bayes classifier are no better than random chance.


