---
output:
  xaringan::moon_reader:
    css: ["default", "styling/sydney-fonts.css", "styling/sydney.css"]
    self_contained: false
    seal: true
    nature:
      beforeInit: ["styling/remark-zoom.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
      navigation:
        scroll: false
---

class: title-slide
background-image: url("styling/USydLogo-black.svg"), url("styling/title-image1.jpg")
background-position: 10% 90%, 100% 50%
background-size: 160px, 100% 100%

.content-box-purple[
# .black[Activity 2]
## Performance Evaluation of Classifications
### Dario Strbenac
### 29 June 2018
]

---

## Activity Overview

- Feature selection stability.

--

- Evaluation of overall error, sample-specific error, precision, recall.

--

- Comparison of various classifiers.

---

```{r, echo = FALSE}
library(knitr)
opts_knit[["set"]](root.dir = "/home/dario/Documents/tutorial/")
load("/home/dario/Documents/tutorial/data/classified.RData") # From AMLclassifiers.html tutorial.
```

---

## Feature Selection Stability

- If the genes being selected are the same ones in most of the cross-validations, then the classifier has good stability.

--

- `selectionPlot` provides a way to compare all pairs of gene selections within a classifier or between classifiers.

--

- Input is a list of `ClassifyResult` objects.

--

-  For 20 permutations and 5 folds, there are $^{100}C_2 = 4950$ overlaps to compute. Like `runTests`, `selectionPlot` may utilise multiple processors.

```{r, message = FALSE, echo = FALSE}
library(ClassifyR)
```

---

## Feature Selection Stability

```{r, fig.align = "center", fig.width = 14, fig.height = 3}
library(ggplot2)
margins <- unit(c(1, 1, 1, 1), "lines")
withinChoices <- selectionPlot(list(classifiedDM, classifiedDD),
                               xVariable = "selectionName", xLabel = "Selection Method",
                               columnVariable = "None",
                               boxFillColouring = "None", boxLineColouring = "None",
                               rotate90 = TRUE, margin = margins)
```

About a quarter of the features selected are typically the same between cross-validations.

---

## Changing Elements of Saved Plots

- Almost all plots produced by ClassifyR are `ggplot` objects created by ggplot2.

- Such objects can be customised after creation because ggplot2 uses a 'painting-over' graphics model, unlike base R graphics.

--

```{r, fig.align = "center", fig.width = 14, fig.height = 4}
withinChoices + ggtitle("Chosen Genes Overlaps")
```

---

## Error / Accuracy of Predictions

- `calcCVperformance` calculates performance metrics for `ClassifyResult` objects. 12 different metrics can be calculated.

--

- Metrics are all applicable to data sets with two *or more* classes.

--

- `calcExternalPerformance` can be used on a pair of factor vectors of the same length.

--

```{r}
actualClasses <- factor(c("Yes", "Yes", "No", "No", "No"))
predictedClasses <- factor(c("Yes", "No", "No", "No", "No"))
calcExternalPerformance(actualClasses, predictedClasses, "error")
calcExternalPerformance(actualClasses, predictedClasses, "accuracy")
```

---

## Balanced Error Rate for Resistance Classification

- Class sizes of resistance data set are imbalanced. Errors should be summarised by the balanced error rate.

--

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "balanced error")
classifiedDM
classifiedDD <- calcCVperformance(classifiedDD, "balanced error")
classifiedClinical <- calcCVperformance(classifiedClinical, "balanced error")
```

---

## Distribution of Balanced Error Rate

The set of samples was predicted 20 times. `performancePlot` can be used to plot the distribution of a metric to see its stability.

.font70[
```{r, fig.width = 14, fig.height = 4}
errorPlot <- performancePlot(list(classifiedDM, classifiedDD, classifiedClinical),
                             performanceName = "Balanced Error Rate",
                             boxFillColouring = "None", boxLineColouring = "None",
                             columnVariable = "None", title = "Balanced Errors",
                             xLabel = "Classifier", rotate90 = TRUE, margin = margins,
                             plot = FALSE)
errorPlot + geom_hline(yintercept = 0.5, colour = "red")
```
]

---

## Sample-specific Error Rate

Calculate the sample-specific error rates for each patient.

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "sample error")
classifiedDD <- calcCVperformance(classifiedDD, "sample error")
classifiedClinical <- calcCVperformance(classifiedClinical, "sample error")
classifiedClinical
```

---

## Sample-specific Error Rate

Plot a heatmap of errors using `samplesMetricMap`.

```{r, fig.width = 14, fig.height = 3.8}
errorPlot <- samplesMetricMap(list(classifiedDM, classifiedDD, classifiedClinical),
                              xAxisLabel = "Samples", yAxisLabel = "Classifier",
                              showXtickLabels = FALSE)
```

DLDA is the only method which has a similar error profile in the minority and majority class.

---

## Precision, Recall, F1 Score

- Micro and macro versions of these can be similarly calculated to the error rates demonstrated previously.

--

- Use the macro version because each class makes an equal contribution to the metric, unlike for the micro version.

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "macro precision")
performance(classifiedDM)[["Macro Precision"]]
```

